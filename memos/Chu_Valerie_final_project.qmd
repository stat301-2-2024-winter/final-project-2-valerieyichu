---
title: "An Analysis of Avocados"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Valerie Chu"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-valerieyichu.git](https://github.com/stat301-2-2024-winter/final-project-2-valerieyichu.git)

:::


```{r}
#| echo: false

# load packages
library(tidyverse)
library(tidymodels)
library(here)

# handle common conflicts
tidymodels_prefer()

# load packages
load(here("results/avocado_split.rda"))
load(here("results/avocado_recipe_param.rda"))
load(here("results/avocado_recipe_tree.rda"))
load(here("results/fit_null.rda"))
load(here("results/fit_lm.rda"))
load(here("results/tuned_lasso.rda"))
load(here("results/tuned_ridge.rda"))
load(here("results/tuned_bt.rda"))
load(here("results/tuned_knn.rda"))
load(here("results/tuned_rf.rda"))
load(here("results/tbl_result.rda"))

load(here("results/avocado_recipe_param_2.rda"))
load(here("results/avocado_recipe_tree_2.rda"))
load(here("results/fit_null_2.rda"))
load(here("results/fit_lm_2.rda"))
load(here("results/tuned_lasso_2.rda"))
load(here("results/tuned_ridge_2.rda"))
load(here("results/tuned_bt_2.rda"))
load(here("results/tuned_knn_2.rda"))
load(here("results/tuned_rf_2.rda"))
load(here("results/tbl_result_2.rda"))

```


```{r, echo = FALSE}
# Read in the avocado dataset
avocado <- read_csv(here("data/avocado.csv")) |> 
  janitor::clean_names()

```

## Introduction

I'm Californian, and I love avocados. There's avocado toast, avocado spread, avocado in salad, and much more. That's why I was interested in a dataset about avocados. Hopefully, this final project will help me pinpoint and eventually move to a city with the cheapest avocados...

As such, I want to predict the average price of a single avocado (`average_price`). Training and selecting a model that best predicts average price based on a number of variables will help me pin down the average price of a single avocado in the places I want to live in. 

The data I will be using to build my predictive model comes from Kaggle. 

This dataset contains historical data on avocado prices and sales volume in multiple US markets.

This is the link to the Avocado Prices dataset on Kaggle:
**[https://www.kaggle.com/datasets/neuromusic/avocado-prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices)** ^[Kiggins, Justin. "Avocado Prices." Kaggle, 2018, https://www.kaggle.com/datasets/neuromusic/avocado-prices.]

This data was downloaded from the Hass Avocado Board website ^[Hass Avocado Board, https://hassavocadoboard.com/] in May of 2018 and compiled into a single CSV. There are 18,249 observations in this dataset and 14 variables. There is 1 date-time object, 10 numerical variables, 2 categorical variables and 1 id column. There is no missingness in this dataset. 

To reiterate, **I want to predict the price a single avocado will sell at (ie. `average_price`).** This is a **regression problem**. 

These are the definitions the Kaggle site gives for some of the variables in the avocado dataset: 

- `date` - The date of the observation

- `average_price` - the average price of a single avocado

- `type` - conventional or organic

- `year` - the year

- `region` - the city or region of the observation

- `total_volume` - Total number of avocados sold

- `x4046` - Total number of avocados with PLU 4046 sold

- `x4225` - Total number of avocados with PLU 4225 sold

- `x4770` - Total number of avocados with PLU 4770 sold


## Data Overview

Let's start with a check for missingness: 

```{r}
#| echo: false

naniar::gg_miss_var(avocado)

```

**There is no missingness in this dataset.**

A skim of this dataset also confirms this:
```{r}
#| echo: false

skimr::skim(avocado)

```

<br/> Next, let's explore our target variable, `average_price`: 

```{r}
#| echo: false

avocado |> 
  ggplot(aes(x = average_price)) +
  geom_density() +
  scale_x_continuous(breaks = seq(0, 4, 0.5)) +
  theme_minimal()

```

A density plot above shows that average price is slightly right skewed, but not to the extreme that it warrants any logarithmic transformations. The average price is between $1 and $1.5. Now, let's get a bit more specific with a boxplot:

```{r}
#| echo: false

avocado |> 
  ggplot(aes(x = average_price)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(0, 4, 0.5)) +
  theme_minimal()

```


The median `average_price` of an avocado in this dataset is $1.37. The IQR is about $0.56. This boxplot is slightly right skewed, but not to the extreme that it warrants any logarithmic transformations. There are several outliers for `average_price`, with these outliers spanning the $2.50 to $3.25 per avocado range. The cheapest average price of an avocado is $0.44. The most expensive average price of an avocado is $3.25.

```{r}
#| echo: false

avocado |> 
  select(average_price) |> 
  summary(mean = mean(average_price, na.rm = TRUE),
          median = median(average_price, na.rm = TRUE)) |> 
  knitr::kable()

```


<br/> Now, let's create a correlation plot to look at the relationship between average price and the predictor variables, as well as the relationship between variables:

```{r}
#| echo: false

avocado |> 
  select(average_price, total_volume, x4046, x4225, x4770, 
         total_bags, small_bags, large_bags,  x_large_bags) |> 
  cor() |> 
  knitr::kable()

```


<br/> Pointing out some things I'll note when explaining my feature engineered recipe, but that are worth highlighting from this correlation plot:

- There is a strong correlation between the size of the avocado (x4046, x4225, x4770 denote the type of avocado, where each weight category is given a specific number) and the size of the bag (small bags, large bags, extra large bags). This is unsurprising, yet also interesting to consider because by definition, the weight categories overlap a bit ^[https://loveonetoday.com/how-to/identify-hass-avocados/], so that's why the correlations aren't perfect. 

- It's also interesting to note that what I had originally thought would be a great interaction — between average price and total volume (the total number of avocados sold) — had almost no correlation. But maybe average price will be best predicted by a combination of these variables, instead of any one variable alone. 


## Methods

### Data Splitting Procedure
This is a regression problem, so the assessment metric I will be using is the RMSE. RMSE calculates the average difference between the model's predicted prices and the actual prices, and it better accounts for outliers, which is important since a preliminary exploration of average price reveals that this variable is *slightly* skewed right. 

Because this dataset is quite large, at 18,249 observations, I decided to use 70% of this dataset to train the model (12,772 observations) and 30% to test the model (5,477 observations). 


### Resampling Technique

I also resampled my dataset, partitioning the dataset into 10 parts and repeating this 5 more times. Training the model like this across multiple subsets of data lets us measure the average performance across all these partitions. This lets us better quantify the variability of our data and get better estimates, while also preventing overfitting. So 1,277 observations will be in the assessment set and 11,494 observations will be in the training set.  


### Model Types

I used the following model types: Null (baseline model), OLS, Lasso, Ridge, Boosted Tree, K Nearest Neighbors, and Random Forest. 

- The null model is the simplest possible baseline model. I am using it a benchmark/reference model to help me ultimately decide whether building increasingly complex models would be worth the effort and expense. There is nothing to tune for this model. 

- The ordinary linear regression model is a simple parametric model that uses the traditional method of least squares to solve for model parameters. I am using it as simple model to compare other models against. There is nothing to tune for this model. 

- The lasso model is a regularized regression, parametric model with a mixture of 1 that adds a penalty term to standard regression. I will compare this model against other models to determine the best one. I tuned the penalty for this model.

- The ridge model a regularized regression, parametric model with a mixture of 0 that adds a penalty term to standard regression. I will compare this model against other models to determine the best one. I tuned the penalty for this model.

- The boosted tree model is a non-parametric model where multiple sequential simple regression trees are combined into a stronger model, and each tree is trained on the residuals from the previous sequence of trees. All trees are then combined together using an additive model, whose weights are estimated via the gradient descent procedure. I will compare this model against other models to determine the best one. I tuned the mtry, min_n, and learn_rate for this model. 

- The k nearest neighbors model is a non-parametric model that uses the K most similar data points from the training set to predict new samples. I will compare this model against other models to determine the best one. I tuned the neighbors for this model. 

- The random forest model is a non-parametric model that creates a large number of decision trees, each independent of the others. The final prediction uses all predictions from the individual trees and combines them, making this model type resistant to outliers. I will compare this model against other models to determine the best one. I tuned the mtry and min_n for this model. 


### Recipes

I will be using a kitchen sink recipe and a feature engineered recipe. 


For the kitchen sink recipes, my goal was to create a very simple, general recipe. 

- I started by predicting the target variable with all other variables. 

- I removed the id variable, date, and region. This is because the id variable will perfectly predict each observation which isn't useful. I removed date because the useful information is already in the year variable, so it's redundant to keep it in. I removed region because there would be 54 factored levels and that took too much computational time. 

- I converted type and region (the factor variables) into numeric binary model terms corresponding to the levels of the original data.

- I filtered out variables with near zero variance. 

- Then I centered and scaled all predictors. 


I decided on my feature engineered recipes based on my exploratory data analysis of the target variable, average price, and other variables in the dataset. See `1_exploration_fe` for more information.

- I removed the id variable, date, and region for the same reasons as the kitchen sink recipe. 

- I converted type and region (the factor variables) into numeric binary model terms corresponding to the levels of the original data.

- This time, I added interaction terms. There is an interaction term between the type of avocado and the total number of avocados sold, because especially with the rise of an organic-conscious craze in recent years, I think the interaction between the type of avocado and the total number of avocados sold will have an effect on average price. There is also an interaction term between the avocado type 4770 and extra large bags. This is due to two reasons. First, the Hass Avocado Board ^[https://loveonetoday.com/how-to/identify-hass-avocados/] describes avocado PLU 4770 as extra large avocados. That means there should be a relationship between it and extra large bags, potentially with some avocado PLU 4225 mixed up in there due to 4225 being large avocados. Second, the relationship between avocado type 4770 and extra large bags is non linear. That means any interaction term created with them will *not* simply result in almost identical similar constants, so that could help improve our modeling accuracy. 

- I filtered out variables with near zero variance. 

- Then I centered and scaled all predictors. 


A few additional notes on my feature engineered recipes: 

- There is no missingness in my data, so I don't need to impute. 

- I did not log transform anything because several of my predictor values had zeros, so transforming will produce NaN values and I don't want that. Also, the distribution of average price looked fine and didn't warrant a log transformation. 

- I was going to consider adding a natural spline basis function to my featured engineered recipes, but the relationships between the predictor variables and average price seem to be pretty linear, and I don't see the need for it unless I run the risk of overfitting. 


## Model Building & Selection




## Final Model Analysis

The table of RMSEs and standard errors created below was made using kitchen recipes. It displays the RMSE and standard error for the model with the lowest RMSE for each model type.

The baseline is the null model. I've also defined and fitted the OLS, Lasso, Ridge, Boosted Tree, K Nearest Neighbors, and Random Forest models. 

```{r, echo = FALSE}
tbl_result |> 
  knitr::kable()

```

The best model is the random forest model because it has the lowest RMSE. It also has a low standard error. So the predictions created by the random forest model should be the closest to the true values.

### EDA

Next, I will do a preliminary EDA to continue on the progress made in Progress Memo 1. This will help decide what interactions to use in my second recipe. 

Below is a correlation plot.

```{r, echo = FALSE}
avocado |> 
  select(average_price, total_volume, x4046, x4225, x4770, total_bags, 
         small_bags, large_bags, x_large_bags) |> 
  cor() |> 
  knitr::kable()
```
It seems like for my second recipe, I could create interactions between `total_bags` and `small_bags`, `total_volume` and `x4046`, and `total_volume` and `x4225`. (As a reminder, 4046 and 4225 are PLU numbers — numbers used to properly identify avocados sold at retail.)


## Conclusion
 

## References


## Appendix A: An exploration of average avocado price 

