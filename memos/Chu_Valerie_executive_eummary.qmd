---
title: "An Analysis of Avocados"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Valerie Chu"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-valerieyichu.git](https://github.com/stat301-2-2024-winter/final-project-2-valerieyichu.git)

:::


```{r}
#| echo: false

# load packages
library(tidyverse)
library(tidymodels)
library(here)

# handle common conflicts
tidymodels_prefer()

# load packages
load(here("results/avocado_split.rda"))
load(here("results/avocado_recipe_param.rda"))
load(here("results/avocado_recipe_tree.rda"))
load(here("results/fit_null.rda"))
load(here("results/fit_lm.rda"))
load(here("results/tuned_lasso.rda"))
load(here("results/tuned_ridge.rda"))
load(here("results/tuned_bt.rda"))
load(here("results/tuned_knn.rda"))
load(here("results/tuned_rf.rda"))
load(here("results/tbl_result.rda"))

load(here("results/avocado_recipe_param_2.rda"))
load(here("results/avocado_recipe_tree_2.rda"))
load(here("results/fit_null_2.rda"))
load(here("results/fit_lm_2.rda"))
load(here("results/tuned_lasso_2.rda"))
load(here("results/tuned_ridge_2.rda"))
load(here("results/tuned_bt_2.rda"))
load(here("results/tuned_knn_2.rda"))
load(here("results/tuned_rf_2.rda"))
load(here("results/tbl_result_2.rda"))

load(here("results/metrics_final_fit.rda"))
load(here("results/final_pred_plot.rda"))

```


```{r, echo = FALSE}
# Read in the avocado dataset
avocado <- read_csv(here("data/avocado.csv")) |> 
  janitor::clean_names()

```

## Introduction

I'm Californian, and I love avocados. There's avocado toast, avocado spread, avocado in salad, and much more. That's why I was interested in a dataset about avocados. Hopefully, this final project will help me pinpoint and eventually move to a city with the cheapest avocados...

As such, I want to predict the average price of a single avocado (`average_price`). Training and selecting a model that best predicts average price based on a number of variables will help me pin down the average price of a single avocado in the places I want to live in. 

The data I will be using to build my predictive model comes from Kaggle. 

This dataset contains historical data on avocado prices and sales volume in multiple US markets.

This is the link to the Avocado Prices dataset on Kaggle:
**[https://www.kaggle.com/datasets/neuromusic/avocado-prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices)** ^[Kiggins, Justin. "Avocado Prices." Kaggle, 2018, https://www.kaggle.com/datasets/neuromusic/avocado-prices.]

This data was downloaded from the Hass Avocado Board website ^[Hass Avocado Board, https://hassavocadoboard.com/] in May of 2018 and compiled into a single CSV. There are 18,249 observations in this dataset and 14 variables. There is 1 date-time object, 10 numerical variables, 2 categorical variables and 1 id column. There is no missingness in this dataset. 

To reiterate, **I want to predict the price a single avocado will sell at (ie. `average_price`).** This is a **regression problem**. 

These are the definitions the Kaggle site gives for some of the variables in the avocado dataset: 

- `date` - The date of the observation

- `average_price` - the average price of a single avocado

- `type` - conventional or organic

- `year` - the year

- `region` - the city or region of the observation

- `total_volume` - Total number of avocados sold

- `x4046` - Total number of avocados with PLU 4046 sold

- `x4225` - Total number of avocados with PLU 4225 sold

- `x4770` - Total number of avocados with PLU 4770 sold


## Data Overview

Let's start with a check for missingness, in @fig-missingness-naniar: 

```{r}
#| echo: false
#| label: fig-missingness-naniar
#| fig-cap: "Are there any variables missing from this dataset?"

naniar::gg_miss_var(avocado)

```

**There is no missingness in this dataset.**

A skim of this dataset in @fig-skim also confirms this:
```{r}
#| echo: false
#| label: fig-skim
#| fig-cap: "A skim of the dataset"

skimr::skim(avocado)

```

<br/> Next, let's explore our target variable, `average_price`, in @fig-density: 

```{r}
#| echo: false
#| label: fig-density
#| fig-cap: "What is the distribution of the average price of a single avocado?"

avocado |> 
  ggplot(aes(x = average_price)) +
  geom_density() +
  scale_x_continuous(breaks = seq(0, 4, 0.5)) +
  theme_minimal()

```

@fig-density shows that average price is slightly right skewed, but not to the extreme that it warrants any logarithmic transformations. The average price is between $1 and $1.5. Now, let's get a bit more specific with a boxplot, in @fig-boxplot:

```{r}
#| echo: false
#| label: fig-boxplot
#| fig-cap: "What is the distribution of the average price of a single avocado?"

avocado |> 
  ggplot(aes(x = average_price)) +
  geom_boxplot() +
  scale_x_continuous(breaks = seq(0, 4, 0.5)) +
  theme_minimal()

```


From @fig-boxplot and @tbl-average-price, we can see that: The median `average_price` of an avocado in this dataset is $1.37. The IQR is about $0.56. This boxplot is slightly right skewed, but not to the extreme that it warrants any logarithmic transformations. There are several outliers for `average_price`, with these outliers spanning the $2.50 to $3.25 per avocado range. The cheapest average price of an avocado is $0.44. The most expensive average price of an avocado is $3.25.

```{r}
#| echo: false
#| label: tbl-average-price
#| tbl-cap: "What is the distribution of the average price of a single avocado?"

avocado |> 
  select(average_price) |> 
  summary(mean = mean(average_price, na.rm = TRUE),
          median = median(average_price, na.rm = TRUE)) |> 
  knitr::kable()

```


<br/> Now, let's create a correlation plot, @tbl-correlation, to look at the relationship between average price and the predictor variables, as well as the relationship between variables:

```{r}
#| echo: false
#| label: tbl-correlation
#| tbl-cap: "A correlation plot"

avocado |> 
  select(average_price, total_volume, x4046, x4225, x4770, 
         total_bags, small_bags, large_bags,  x_large_bags) |> 
  cor() |> 
  knitr::kable()

```


<br/> Pointing out some things I'll note when explaining my feature engineered recipe, but that are worth highlighting from @tbl-correlation:

- There is a strong correlation between the size of the avocado (x4046, x4225, x4770 denote the type of avocado, where each weight category is given a specific number) and the size of the bag (small bags, large bags, extra large bags). This is unsurprising, yet also interesting to consider because by definition, the weight categories overlap a bit ^[https://loveonetoday.com/how-to/identify-hass-avocados/], so that's why the correlations aren't perfect. 

- It's also interesting to note that what I had originally thought would be a great interaction — between average price and total volume (the total number of avocados sold) — had almost no correlation. But maybe average price will be best predicted by a combination of these variables, instead of any one variable alone. 


The last thing I want to note for this data overview section is that further exploration (ex. such as exploring relationships and transformations) can be found in `1_exploration_fe`. This additional exploratory data analysis informed how I built my feature engineered recipes, and the comments I put in the R file explain my thought process in additional detail beyond what I have here.



## Methods

### Data Splitting Procedure
This is a regression problem, so the assessment metric I will be using is the RMSE. RMSE calculates the average difference between the model's predicted prices and the actual prices, and it better accounts for outliers, which is important since a preliminary exploration of average price reveals that this variable is *slightly* skewed right. 

Because this dataset is quite large, at 18,249 observations, I decided to use 70% of this dataset to train the model (12,772 observations) and 30% to test the model (5,477 observations). 


### Resampling Technique

I also resampled my dataset, partitioning the dataset into 10 parts and repeating this 5 more times. Training the model like this across multiple subsets of data lets us measure the average performance across all these partitions. This lets us better quantify the variability of our data and get better estimates, while also preventing overfitting. So 1,277 observations will be in the assessment set and 11,494 observations will be in the training set.  


### Model Types

I used the following model types: Null (baseline model), OLS, Lasso, Ridge, Boosted Tree, K Nearest Neighbors, and Random Forest. 

- The null model is the simplest possible baseline model. I am using it a benchmark/reference model to help me ultimately decide whether building increasingly complex models would be worth the effort and expense. There is nothing to tune for this model. 

- The ordinary linear regression model is a simple parametric model that uses the traditional method of least squares to solve for model parameters. I am using it as simple model to compare other models against. There is nothing to tune for this model. 

- The lasso model is a regularized regression, parametric model with a mixture of 1 that adds a penalty term to standard regression. I will compare this model against other models to determine the best one. I tuned the penalty for this model.

- The ridge model a regularized regression, parametric model with a mixture of 0 that adds a penalty term to standard regression. I will compare this model against other models to determine the best one. I tuned the penalty for this model.

- The boosted tree model is a non-parametric model where multiple sequential simple regression trees are combined into a stronger model, and each tree is trained on the residuals from the previous sequence of trees. All trees are then combined together using an additive model, whose weights are estimated via the gradient descent procedure. I will compare this model against other models to determine the best one. I tuned the mtry, min_n, and learn_rate for this model. 

- The k nearest neighbors model is a non-parametric model that uses the K most similar data points from the training set to predict new samples. I will compare this model against other models to determine the best one. I tuned the neighbors for this model. 

- The random forest model is a non-parametric model that creates a large number of decision trees, each independent of the others. The final prediction uses all predictions from the individual trees and combines them, making this model type resistant to outliers. I will compare this model against other models to determine the best one. I tuned the mtry and min_n for this model. 


### Recipes

I will be using a kitchen sink recipe and a feature engineered recipe. 


For the kitchen sink recipes, my goal was to create a very simple, general recipe. 

- I started by predicting the target variable with all other variables. 

- I removed the id variable, date, and region. This is because the id variable will perfectly predict each observation which isn't useful. I removed date because the useful information is already in the year variable, so it's redundant to keep it in. I removed region because there would be 54 factored levels and that took too much computational time. 

- I converted type and region (the factor variables) into numeric binary model terms corresponding to the levels of the original data.

- I filtered out variables with near zero variance. 

- Then I centered and scaled all predictors. 


I decided on my feature engineered recipes based on my exploratory data analysis of the target variable, average price, and other variables in the dataset. See `1_exploration_fe` for more information.

- I removed the id variable, date, and region for the same reasons as the kitchen sink recipe. 

- I converted type and region (the factor variables) into numeric binary model terms corresponding to the levels of the original data.

- This time, I added interaction terms. There is an interaction term between the type of avocado and the total number of avocados sold, because especially with the rise of an organic-conscious craze in recent years, I think the interaction between the type of avocado and the total number of avocados sold will have an effect on average price. There is also an interaction term between the avocado type 4770 and extra large bags. This is due to two reasons. First, the Hass Avocado Board ^[https://loveonetoday.com/how-to/identify-hass-avocados/] describes avocado PLU 4770 as extra large avocados. That means there should be a relationship between it and extra large bags, potentially with some avocado PLU 4225 mixed up in there due to 4225 being large avocados. Second, the relationship between avocado type 4770 and extra large bags is non linear. That means any interaction term created with them will *not* simply result in almost identical similar constants, so that could help improve our modeling accuracy. 

- I filtered out variables with near zero variance. 

- Then I centered and scaled all predictors. 


A few additional notes on my feature engineered recipes: 

- There is no missingness in my data, so I don't need to impute. 

- I did not log transform anything because several of my predictor values had zeros, so transforming will produce NaN values and I don't want that. Also, the distribution of average price looked fine and didn't warrant a log transformation. 

- I was going to consider adding a natural spline basis function to my featured engineered recipes, but the relationships between the predictor variables and average price seem to be pretty linear, and I don't see the need for it unless I run the risk of overfitting. 


## Model Building & Selection

To reiterate, I am using RMSE as my assessment metric, the metric that will be used to compare models and determine which will be the final/winning model.

### Final/Winning Model

@tbl-models shows the model results: 

```{r, echo = FALSE}
#| label: tbl-models
#| tbl-cap: "The RMSE and Standard Errors of All Models"

tbl_result_large <- bind_rows(tbl_result, tbl_result_2)

tbl_result_large |> 
  knitr::kable()

```

@tbl-models indicates that building more complex models only somewhat appears to be worthwhile. 

- The feature engineered non-parametric models (random forest, boosted tree, k nearest neighbors) performed worse than than their kitchen sink counterparts. 

- However, the feature engineered parametric models (ordinary linear regression, lasso, and ridge) performed *better* than their kitchen sink counterparts — the feature engineered parametric models had lower RMSEs and lower standard errors, their their predictions were closer to the true values. 

- And of course, the null models built with both the kitchen sink and feature engineered recipes produced the exact same RMSE and standard error as expected, because the recipes do not affect the null model; that's why it's a good baseline model. 

Still, the random forest model built with the kitchen sink had the lowest RMSE and lowest standard error of all the models, so it is the best model. Its predictions will be closest to the true values.

### Tuning Parameters

Below are a series of autoplots. They show what the best hyperparameter for each model type is. 

The left column shows the autoplots from the models produced using the kitchen sink recipes. The right column shows the autoplots from the models produced using the feature engineered recipes. 

```{r, echo = FALSE}
#| label: autoplots
#| fig-show: "hold"
#| out-width: "100%"
#| layout-ncol: 2

autoplot_lasso <- autoplot(tuned_lasso, metric = "rmse") +
  labs(title = "Lasso") +
  theme_minimal()
# A penalty of 1e^-07 leads to the lowest RMSE.  


autoplot_ridge <- autoplot(tuned_ridge, metric = "rmse") +
  labs(title = "Ridge") +
  theme_minimal()
# A penalty of 1e^-07 leads to the lowest RMSE. 


autoplot_bt <- autoplot(tuned_bt, metric = "rmse") +
  labs(title = "Boosted Tree") +
  theme_minimal()
# A learn rate of 0.630957 leads to the lowest RMSE. 
# A mtry ("# Randomly Selected Predictors") of 14 leads to the lowest RMSE. 
# A min_n ("Minimal Node Size") of 14 leads to the lowest RMSE. 


autoplot_knn <- autoplot(tuned_knn, metric = "rmse") +
  labs(title = "K Nearest Neighbors") +
  theme_minimal()
# A neighbors of 8 leads to the lowest RMSE. 


autoplot_rf <- autoplot(tuned_rf, metric = "rmse") +
  labs(title = "Random Forest") +
  theme_minimal()
# A mtry of 7 leads to the lowest RMSE. 
# A min_n of 2 leads to the lowest RMSE.



autoplot_lasso_2 <- autoplot(tuned_lasso_2, metric = "rmse") +
  labs(title = "Lasso") +
  theme_minimal()
# A penalty of 1e^-08 leads to the lowest RMSE.  


autoplot_ridge_2 <- autoplot(tuned_ridge_2, metric = "rmse") +
  labs(title = "Ridge") +
  theme_minimal()
# A penalty of 1e^-08 leads to the lowest RMSE. 


autoplot_bt_2 <- autoplot(tuned_bt_2, metric = "rmse") +
  labs(title = "Boosted Tree") +
  theme_minimal()
# A learn rate of 0.630957 leads to the lowest RMSE. 
# A mtry ("# Randomly Selected Predictors") of 19 leads to the lowest RMSE. 
# A min_n ("Minimal Node Size") of 17 leads to the lowest RMSE. 


autoplot_knn_2 <- autoplot(tuned_knn_2, metric = "rmse") +
  labs(title = "K Nearest Neighbors") +
  theme_minimal()
# A neighbors of 15 leads to the lowest RMSE. 


autoplot_rf_2 <- autoplot(tuned_rf_2, metric = "rmse") +
  labs(title = "Random Forest") +
  theme_minimal()
# A mtry of 9 leads to the lowest RMSE. 
# A min_n of 2 leads to the lowest RMSE.

autoplot_lasso
autoplot_lasso_2
autoplot_ridge
autoplot_ridge_2
autoplot_bt
autoplot_bt_2
autoplot_knn
autoplot_knn_2
autoplot_rf
autoplot_rf_2

```

Based on the autoplots above and the select_best function, these are the tuning parameter combinations with the best performance values for each model type: 


The best hyperparameter for the kitchen sink lasso model is a penalty of 0.0000000001

```{r, echo = FALSE}
tuned_lasso |> select_best(metric = "rmse") |> 
  knitr::kable()
# Best hyperparameter: Penalty = 0.0000000001

```
<br/>


The best hyperparameter for the kitchen sink ridge model is a penalty of 0.0000000001

```{r, echo = FALSE}
tuned_ridge |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: Penalty = 0.0000000001
```
<br/>


The best hyperparameters for the kitchen sink boosted model are mtry = 14, min_n = 14, learn_rate = 0.631

```{r, echo = FALSE}
tuned_bt |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameters: mtry = 14. min_n = 14. learn_rate = 0.631
```
<br/>


The best hyperparameter for the kitchen sink k nearest neighbors model is neighbors = 8

```{r, echo = FALSE}
tuned_knn |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: neighbors = 8
```
<br/>


The best hyperparameters for the kitchen sink random forest model are mtry = 7, min_n = 2

```{r, echo = FALSE}
tuned_rf |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: mtry = 7. min_n = 2
```
<br/>


The best hyperparameter for the feature engineered lasso model is a penalty of 0.0000000001

```{r, echo = FALSE}
tuned_lasso_2 |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: Penalty = 0.0000000001
```
<br/>


The best hyperparameter for the feature engineered ridge model is a penalty of 0.0000000001

```{r, echo = FALSE}
tuned_ridge_2 |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: Penalty = 0.0000000001
```
<br/>


The best hyperparameters for the feature engineered boosted tree model are mtry = 17, min_n = 9, learn_rate = 0.631

```{r, echo = FALSE}
tuned_bt_2 |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameters: mtry = 17. min_n = 9. learn_rate = 0.631
```
<br/>


The best hyperparameters for the feature engineered k nearest neighbors model are neighbors = 15

```{r, echo = FALSE}
tuned_knn_2 |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: neighbors = 15
```
<br/>


The best hyperparameters for the feature engineered random forest model are mtry = 9, min_n = 2

```{r, echo = FALSE}
tuned_rf_2 |> select_best(metric = "rmse")|> 
  knitr::kable()
# Best hyperparameter: mtry = 9. min_n = 2
```
<br/>


We know that the kitchen sink random forest model is the best model because it has the lowest RMSE and a low standard error. As stated above, its best hyperparameters are mtry = 7 and min_n = 2.

If we wanted to explore further tuning, we could look again at the autoplot. We see that as the minimal node size decreases, RMSE decreases. So the best min_n should stay at 2. We also see that as the number of randomly selected predictors approaches 7, that is when RMSE is lowest — regardless of the minimal node size. So it seems like there isn't much of a point in tuning again, especially since we already know from the autoplot that these two hyperparameters will optimize the model such that it produces the lowest RMSE. 

There were some interesting similarities between the best hyperparameters for each model type:

- For the kitchen sink and feature engineered lasso and ridge models (four models), the best hyperparameter was a penalty of 0.0000000001. This is likely because a lower penalty will less heavily penalize large coefficients, which will let the model be more flexible. This is probably why the feature engineered parametric models did better than the kitchen sink parametric models. 

- The best hyperparameters for the non-parametric feature engineered recipes tended to be a lot higher than the best hyperparameters for the non-parametric kitchen sink recipes. Still, even with more nearest neighbors, more randomly selected predictors, and a higher minimal node size to draw from, the non-parametric feature engineered recipe did worse than the non-parametric kitchen sink recipe. Overfitting could be an explanation for this. 

Overall, I'm surprised that the random forest model built with the kitchen sink recipe won, but I'm not surprised that one of the random forest models won. The avocado dataset is relatively unique in that certain variables have particularly strong correlations with each other while certain variables have weak correlations with each other. This means that if predictions are done in sequentially like in a boosted tree, or if avocado prices are predicted based off others most similar to them like in a k nearest neighbors model, the end result might not be too accurate. Also, I thought a non-parametric model might win because it would be more flexible than a parametric model. And a random forest model also creates a large number of decision trees, each independent of the others, which I thought would work best with this dataset. I was right, for the most part. I think I might have just overfit the feature engineered random forest model. 



## Final Model Analysis

@tbl-metrics-final-fit shows the fit of the final model (the kitchen sink random forest model) to the testing dataset: 

```{r, echo = FALSE}
#| label: tbl-metrics-final-fit
#| tbl-cap: "The Metrics of the Final Model to the Testing Data"

metrics_final_fit |> 
  knitr::kable(digits = c(NA, NA, 4))

```

At 0.1765, the RMSE is low, which is great. The price range for avocados in this dataset are from between $0.44 and $3.25. In the context of the average price of a single avocado, this RMSE would measure the difference between the model’s predicted average price and the actual average price (this difference would be about $0.18). Given that many of the homes in this dataset sold for more than two dollars, a RMSE of only 0.1765 is quite good.

A RSQ of 0.8192 is pretty close to 1, which is quite good. That means around 81.92% of the variation in the average price variable for the testing dataset can be explained by the model. This is not too surprising because our kitchen sink random forest model was the best model, and since we also resampled, we expect our model to perform well.

The MAE is 0.1229, which is decently close to 0, which is good. On average, the absolute difference between the model’s predicted average price and actual average price in the testing dataset is around $0.13. The average price of a single avocado ranges from around $0.44 to around $3.25, so having only about 0.1229 of mean absolute error is low and means our kitchen sink random forest model predictions are good.


@fig-final-pred-plot shows the exploration of predictions vs true values: 

```{r, echo = FALSE}
#| label: fig-final-pred-plot
#| fig-cap: "The Metrics of the Final Model to the Testing Data"

final_pred_plot

```

Overall, the new model fits nicely. Its predictions are a bit high near the upper outliers, but otherwise, it fits the majority of the data well. 

A few notes additional notes about this graph:

- I did not transform the target variable, so there is not need to scale the outcome variable. 

- The effort of building a predictive model does pay off. It is better than the null model — it had a lower RMSE and more accurate predictions. In fact, looking at @tbl-models from the previous section shows that all the models I built were better predictive models than the null model. 

- I think the reason the predictions are a bit higher than they actually are for the upper outliers is because the random forest model is not a parametric model. But it is still great at capturing the majority of the data, probably because its large number of decision trees are independent of each other and thus not overly influenced by how different the avocados could have been to each other. 


## Conclusion

In conclusion, the kitchen sink random forest model was the best predictive model out of all model types (Null (baseline model), OLS, Lasso, Ridge, Boosted Tree, K Nearest Neighbors, and Random Forest) and across both the kitchen sink and feature engineered recipes. 

In the future, I want to boost the predictive accuracy of this model and work with updated data to predict the average price of a single avocado based on the variables in this dataset. I've accomplished my goal of predicting the average price of a single Haas avocado sold between 2015 and 2018, and now I just need an even more accurate model and an even more updated dataset to pin down the city I want to move to in 2026, a city with the cheapest avocados…
 

## References

1. Kiggins, Justin. “Avocado Prices.” Kaggle, 2018, [https://www.kaggle.com/datasets/neuromusic/avocado-prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices).

2. Hass Avocado Board, [https://hassavocadoboard.com/](https://hassavocadoboard.com/)

